{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# IAF\n",
    "- Inverse Autoregressive Flow\n",
    "- 내재 변수의 사후확률 분포 $p_\\phi(z|x)$ 를 자기회귀적으로 생성하여 고차원 내재공간을 효율적으로 학습하는 VAE 파생형 모델이다.\n",
    "- 이제부터 본문은 편의상 영어로, 수식은 LaTeX 문법으로 작성할 것이다.\n",
    "\n",
    "## Citation\n",
    "- [Improving Variational Inference with Inverse Autoregressive Flow (by Diederik P. Kingma, Ilya Sutskever, et. al)](https://arxiv.org/abs/1606.04934)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Z_DIM = 8\n",
    "N_BATCH = 32\n",
    "N_EPOCH = 200\n",
    "LEARNING_RATE = 0.0004\n",
    "R_LOSS_FACTOR = 1000\n",
    "N_THREAD = 2"
   ],
   "metadata": {
    "id": "qzd3SWlM4Ht-",
    "ExecuteTime": {
     "end_time": "2025-10-23T08:23:33.276167Z",
     "start_time": "2025-10-23T08:23:33.272163Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ubQVtyrm3jbu",
    "ExecuteTime": {
     "end_time": "2025-10-23T08:23:34.903179Z",
     "start_time": "2025-10-23T08:23:33.279677Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# For reproducible experiments\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Get a working device\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCKjm5SD4eZM",
    "outputId": "6cbf2596-a151-4763-e631-3f97f02cd9ad",
    "ExecuteTime": {
     "end_time": "2025-10-23T08:23:34.965223Z",
     "start_time": "2025-10-23T08:23:34.959113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Fashion MNIST"
   ],
   "metadata": {
    "id": "r2GOF_fN02zc"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2d2d5dc4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4cefe2ca-43ce-48fd-be3f-97ec6936956c",
    "ExecuteTime": {
     "end_time": "2025-10-23T08:24:35.818539Z",
     "start_time": "2025-10-23T08:23:43.284580Z"
    }
   },
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_data, val_data = random_split(train_data, [0.9, 0.1])\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=N_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=N_THREAD\n",
    ")\n",
    "val_data_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=N_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=N_THREAD\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=N_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=N_THREAD\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"train_data:\", len(train_data_loader.dataset))\n",
    "print(\"val_data:\", len(val_data_loader.dataset))\n",
    "print(\"test_data:\", len(test_data_loader.dataset))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:36<00:00, 730kB/s] \n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 92.8kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:10<00:00, 416kB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 5.15MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_data: 54000\n",
      "val_data: 6000\n",
      "test_data: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper class\n",
    "- **Model**: model abstraction\n",
    "- **Tracker**: save/load model and early stopping\n",
    "- **Trainer**: handles the model training, validation, and testing"
   ],
   "metadata": {
    "id": "egIHWzUj064C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Model(nn.Module, ABC):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return prediction and loss tensor\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_fn(self, *args) -> torch.Tensor:\n",
    "        \"\"\"Return loss tensor\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            pred, _ = self.forward(X)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self, model: Model, save_path=\"./checkpoints\", patience=3, delta=0.001, verbose=True):\n",
    "        self._early_stop = False\n",
    "        self._model = model\n",
    "        self._patience = patience\n",
    "        self._verbose = verbose\n",
    "        self._counter = 0\n",
    "        self._best_score = -np.inf\n",
    "        self._delta = delta\n",
    "        self._save_path = save_path\n",
    "        self._val_loss_min = np.inf\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    def early_stop(self, val_loss, epoch):\n",
    "        score = -val_loss\n",
    "        if score < self._best_score + self._delta:\n",
    "            self._counter += 1\n",
    "            print(f'\\nEarly Stopping counter: {self._counter} out of {self._patience}')\n",
    "            if self._counter >= self._patience:\n",
    "                self._early_stop = True\n",
    "        else:\n",
    "            self._best_score = score\n",
    "            if self._verbose:\n",
    "                print(f'\\nSaving model to checkpoint... (Validate loss: {self._val_loss_min:.5f} --> {val_loss:.5f})')\n",
    "            self.save_checkpoint(epoch=epoch)\n",
    "            self._val_loss_min = val_loss\n",
    "            self._counter = 0\n",
    "        return self._early_stop\n",
    "\n",
    "    def save_checkpoint(self, epoch=None):\n",
    "        model_name = self._model.name\n",
    "        if epoch is None:\n",
    "            filename = f\"{model_name}.pt\"\n",
    "        else:\n",
    "            filename = f\"{model_name}.{epoch}.pt\"\n",
    "        fp = os.path.join(self._save_path, filename)\n",
    "        torch.save(self._model.state_dict(), fp)\n",
    "\n",
    "    def load_checkpoint(self, ckpt_name=None) -> Model:\n",
    "        if ckpt_name is None:\n",
    "            ckpt_name = f\"{self._model.name}.pt\"\n",
    "        ckpt = torch.load(os.path.join(self._save_path, ckpt_name))\n",
    "        self._model.load_state_dict(ckpt)\n",
    "        return self._model\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Model, tracker: Tracker, train_data_loader, val_data_loader, test_data_loader,\n",
    "                 device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.val_data_loader = val_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.tracker = tracker\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        start = time.time()\n",
    "        for epoch in range(1, N_EPOCH + 1):\n",
    "            print(f\"Epoch {epoch}/{N_EPOCH}\\n-------------------------------\")\n",
    "            train_loss = self._train(epoch)\n",
    "            val_loss = self._validate()\n",
    "            early_stop = self.tracker.early_stop(val_loss, epoch)\n",
    "            print(f\"Train loss: {train_loss:.5f}\")\n",
    "            print(f\"Validate loss: {val_loss:.5f}\")\n",
    "            print(\"===============================\\n\")\n",
    "            if early_stop:\n",
    "                print(\"Early stopping now...\")\n",
    "                break\n",
    "        self.tracker.save_checkpoint()\n",
    "        print(f\"Training complete! Elapsed time: {time.time() - start:.1f}s\")\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Evaluate the model's performance.\"\"\"\n",
    "        start = time.time()\n",
    "        loss = self._validate(test=True)\n",
    "        print(f\"\\nTest loss: {loss:.5f} \\n\")\n",
    "        print(f\"Test complete! Elapsed time: {time.time() - start:.1f}s\")\n",
    "\n",
    "    def _train(self, epoch) -> float:\n",
    "        \"\"\"Returns average training loss\"\"\"\n",
    "        dataloader = self.train_data_loader\n",
    "        size = len(dataloader.dataset)\n",
    "        progress = tqdm(enumerate(dataloader), total=int(size / dataloader.batch_size))\n",
    "        loss_sum = 0\n",
    "        self.model.train()\n",
    "        for batch, (X, _) in progress:\n",
    "            X = X.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            pred, loss = self.model(X)\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss = loss.item()\n",
    "            loss_sum += loss\n",
    "            if batch % 100 == 0:\n",
    "                progress.set_description(f\"[Train loss: {loss:.5f}]\")\n",
    "        avg_loss = loss_sum / len(dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def _validate(self, test=False) -> float:\n",
    "        \"\"\"Returns average validate/test loss\"\"\"\n",
    "        dataloader = self.train_data_loader\n",
    "        size = len(dataloader.dataset)\n",
    "        progress = tqdm(enumerate(dataloader), total=int(size / dataloader.batch_size))\n",
    "        self.model.eval()\n",
    "        loss_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, _) in progress:\n",
    "                X = X.to(self.device)\n",
    "                pred, loss = self.model(X)\n",
    "                loss = loss.mean().item()\n",
    "                loss_sum += loss\n",
    "                if batch % 100 == 0:\n",
    "                    if test:\n",
    "                        progress.set_description(f\"[Validate loss: {loss:.5f}]\")\n",
    "                    else:\n",
    "                        progress.set_description(f\"[Test loss: {loss:.5f}]\")\n",
    "        avg_loss = loss_sum / len(dataloader)\n",
    "        return avg_loss\n"
   ],
   "metadata": {
    "id": "TpP0yrpt1E_k"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "id": "qHoC25wZ1FbV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class InverseAutoregressiveFlow(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"IAF\")\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def loss_fn(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "model = InverseAutoregressiveFlow().to(device)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuvQTAXX5DmS",
    "outputId": "d45660e2-7f6d-4f25-b684-fd3ccbc178f4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tracker = Tracker(model=model)\n",
    "trainer = Trainer(model, tracker, train_data_loader, val_data_loader, test_data_loader, device=device)\n",
    "trainer.fit()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "RKRIYNG3iYh4",
    "outputId": "e46fc255-1d4b-4b56-811b-f219fedc81b9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.test()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "58ByLqmHwQPV"
   },
   "cell_type": "markdown",
   "source": "## Load model from disk"
  },
  {
   "metadata": {
    "id": "otOKo2nMxwYy"
   },
   "cell_type": "code",
   "source": [
    "# Load from disk\n",
    "model = tracker.load_checkpoint()\n",
    "model.eval()\n",
    "\n",
    "# Relocate tensors to cpu\n",
    "device = 'cpu'\n",
    "trainer.device = device\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
