{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# IAF\n",
    "- Inverse Autoregressive Flow\n",
    "- 내재 변수의 사후확률 분포 $p_\\phi(z|x)$ 를 자기회귀적으로 생성하여 고차원 내재공간을 효율적으로 학습하는 VAE 파생형 모델이다.\n",
    "- 이제부터 본문은 편의상 영어로, 수식은 LaTeX 문법으로 작성할 것이다.\n",
    "\n",
    "## Citation\n",
    "- [Improving Variational Inference with Inverse Autoregressive Flow (by Diederik P. Kingma, Ilya Sutskever, et. al)](https://arxiv.org/abs/1606.04934)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset options\n",
    "N_BATCH = 32\n",
    "N_THREAD = 2\n",
    "\n",
    "# Learning options\n",
    "N_EPOCH = 40\n",
    "LEARNING_RATE = 0.0005\n",
    "R_LOSS_FACTOR = 1000\n",
    "\n",
    "# Model configuration\n",
    "Z_DIM = 32  # latent space dimension\n",
    "H_DIM = 128  # residual block dimension\n",
    "CONV_OPTIONS = {\n",
    "    'n_layers': 4,\n",
    "    'in_channels': [1, 32, 64, 64],\n",
    "    'out_channels': [32, 64, 64, 64],\n",
    "    'kernel_sizes': [3, 3, 3, 3],\n",
    "    'strides': [1, 2, 2, 1],\n",
    "    'padding': [1, 1, 1, 1],\n",
    "    'feature_shape': (64, 7, 7),\n",
    "}"
   ],
   "metadata": {
    "id": "qzd3SWlM4Ht-",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:13:35.015716Z",
     "start_time": "2025-10-26T01:13:35.012555Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ubQVtyrm3jbu",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:13:35.066201Z",
     "start_time": "2025-10-26T01:13:35.063835Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# For reproducible experiments\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Get a working device\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCKjm5SD4eZM",
    "outputId": "6cbf2596-a151-4763-e631-3f97f02cd9ad",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:13:35.119529Z",
     "start_time": "2025-10-26T01:13:35.115153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Fashion MNIST"
   ],
   "metadata": {
    "id": "r2GOF_fN02zc"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2d2d5dc4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4cefe2ca-43ce-48fd-be3f-97ec6936956c",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:13:36.967015Z",
     "start_time": "2025-10-26T01:13:36.097736Z"
    }
   },
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_data, val_data = random_split(train_data, [0.9, 0.1])\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=N_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=N_THREAD\n",
    ")\n",
    "val_data_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=N_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=N_THREAD\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=N_BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=N_THREAD\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"train_data:\", len(train_data_loader.dataset))\n",
    "print(\"val_data:\", len(val_data_loader.dataset))\n",
    "print(\"test_data:\", len(test_data_loader.dataset))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_data: 54000\n",
      "val_data: 6000\n",
      "test_data: 10000\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper class\n",
    "- **Model**: model abstraction\n",
    "- **Tracker**: save/load model and early stopping\n",
    "- **Trainer**: handles the model training, validation, and testing"
   ],
   "metadata": {
    "id": "egIHWzUj064C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Model(nn.Module, ABC):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return prediction and loss tensor\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_fn(self, *args) -> torch.Tensor:\n",
    "        \"\"\"Return loss tensor\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            pred, _ = self.forward(X)\n",
    "        return pred\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    def __init__(self, model: Model, save_path=\"./checkpoints\", patience=3, delta=0.001, verbose=True):\n",
    "        self._early_stop = False\n",
    "        self._model = model\n",
    "        self._patience = patience\n",
    "        self._verbose = verbose\n",
    "        self._counter = 0\n",
    "        self._best_score = -np.inf\n",
    "        self._delta = delta\n",
    "        self._save_path = save_path\n",
    "        self._val_loss_min = np.inf\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    def early_stop(self, val_loss, epoch):\n",
    "        score = -val_loss\n",
    "        if score < self._best_score + self._delta:\n",
    "            self._counter += 1\n",
    "            print(f'\\nEarly Stopping counter: {self._counter} out of {self._patience}')\n",
    "            if self._counter >= self._patience:\n",
    "                self._early_stop = True\n",
    "        else:\n",
    "            self._best_score = score\n",
    "            if self._verbose:\n",
    "                print(f'\\nSaving model to checkpoint... (Validate loss: {self._val_loss_min:.5f} --> {val_loss:.5f})')\n",
    "            self.save_checkpoint(epoch=epoch)\n",
    "            self._val_loss_min = val_loss\n",
    "            self._counter = 0\n",
    "        return self._early_stop\n",
    "\n",
    "    def save_checkpoint(self, epoch=None):\n",
    "        model_name = self._model.name\n",
    "        if epoch is None:\n",
    "            filename = f\"{model_name}.pt\"\n",
    "        else:\n",
    "            filename = f\"{model_name}.{epoch}.pt\"\n",
    "        fp = os.path.join(self._save_path, filename)\n",
    "        torch.save(self._model.state_dict(), fp)\n",
    "\n",
    "    def load_checkpoint(self, ckpt_name=None) -> Model:\n",
    "        if ckpt_name is None:\n",
    "            ckpt_name = f\"{self._model.name}.pt\"\n",
    "        ckpt = torch.load(os.path.join(self._save_path, ckpt_name))\n",
    "        self._model.load_state_dict(ckpt)\n",
    "        return self._model\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Model, tracker: Tracker, train_data_loader, val_data_loader, test_data_loader,\n",
    "                 device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.val_data_loader = val_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.tracker = tracker\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        start = time.time()\n",
    "        for epoch in range(1, N_EPOCH + 1):\n",
    "            print(f\"Epoch {epoch}/{N_EPOCH}\\n-------------------------------\")\n",
    "            train_loss = self._train(epoch)\n",
    "            val_loss = self._validate()\n",
    "            early_stop = self.tracker.early_stop(val_loss, epoch)\n",
    "            print(f\"Train loss: {train_loss:.5f}\")\n",
    "            print(f\"Validate loss: {val_loss:.5f}\")\n",
    "            print(\"===============================\\n\")\n",
    "            if early_stop:\n",
    "                print(\"Early stopping now...\")\n",
    "                break\n",
    "        self.tracker.save_checkpoint()\n",
    "        print(f\"Training complete! Elapsed time: {time.time() - start:.1f}s\")\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Evaluate the model's performance.\"\"\"\n",
    "        start = time.time()\n",
    "        loss = self._validate(test=True)\n",
    "        print(f\"\\nTest loss: {loss:.5f} \\n\")\n",
    "        print(f\"Test complete! Elapsed time: {time.time() - start:.1f}s\")\n",
    "\n",
    "    def _train(self, epoch) -> float:\n",
    "        \"\"\"Returns average training loss\"\"\"\n",
    "        dataloader = self.train_data_loader\n",
    "        size = len(dataloader.dataset)\n",
    "        progress = tqdm(enumerate(dataloader), total=int(size / dataloader.batch_size))\n",
    "        loss_sum = 0\n",
    "        self.model.train()\n",
    "        for batch, (X, _) in progress:\n",
    "            X = X.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            pred, loss = self.model(X)\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss = loss.item()\n",
    "            loss_sum += loss\n",
    "            if batch % 100 == 0:\n",
    "                progress.set_description(f\"[Train loss: {loss:.5f}]\")\n",
    "        avg_loss = loss_sum / len(dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def _validate(self, test=False) -> float:\n",
    "        \"\"\"Returns average validate/test loss\"\"\"\n",
    "        dataloader = self.train_data_loader\n",
    "        size = len(dataloader.dataset)\n",
    "        progress = tqdm(enumerate(dataloader), total=int(size / dataloader.batch_size))\n",
    "        self.model.eval()\n",
    "        loss_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, _) in progress:\n",
    "                X = X.to(self.device)\n",
    "                pred, loss = self.model(X)\n",
    "                loss = loss.mean().item()\n",
    "                loss_sum += loss\n",
    "                if batch % 100 == 0:\n",
    "                    if test:\n",
    "                        progress.set_description(f\"[Validate loss: {loss:.5f}]\")\n",
    "                    else:\n",
    "                        progress.set_description(f\"[Test loss: {loss:.5f}]\")\n",
    "        avg_loss = loss_sum / len(dataloader)\n",
    "        return avg_loss\n"
   ],
   "metadata": {
    "id": "TpP0yrpt1E_k",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:13:37.626946Z",
     "start_time": "2025-10-26T01:13:37.617707Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "id": "qHoC25wZ1FbV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, conv_options, z_dim: int, h_dim: int, batch_norm=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.drop_out = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.conv_layer = nn.ModuleList()\n",
    "        self.batch_norm = nn.ModuleList()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mu = nn.LazyLinear(self.z_dim)\n",
    "        self.log_var = nn.LazyLinear(self.z_dim)\n",
    "        # self.hidden = nn.LazyLinear(self.h_dim)\n",
    "\n",
    "        for i in range(conv_options['n_layers']):\n",
    "            layer = nn.LazyConv2d(\n",
    "                out_channels=conv_options['out_channels'][i],\n",
    "                kernel_size=conv_options['kernel_sizes'][i],\n",
    "                stride=conv_options['strides'][i],\n",
    "                padding=conv_options['padding'][i]\n",
    "            )\n",
    "            self.conv_layer.append(layer)\n",
    "            if batch_norm:\n",
    "                C = conv_options['out_channels'][i]\n",
    "                self.batch_norm.append(nn.BatchNorm2d(num_features=C))\n",
    "            else:\n",
    "                self.batch_norm.append(nn.Identity())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv2d in enumerate(self.conv_layer):\n",
    "            x = conv2d(x)\n",
    "            x = self.batch_norm[i](x)\n",
    "            x = self.activation(x)\n",
    "            x = self.drop_out(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        # hidden = self.hidden(x)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class GaussianSampler(nn.Module):\n",
    "    def __init__(self, z_dim: int):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    def forward(self, mu, log_var):\n",
    "        epsilon = torch.randn_like(mu)\n",
    "        z = mu + torch.exp(0.5 * log_var) * epsilon\n",
    "        return z\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, conv_options, z_dim: int, batch_norm=True, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.drop_out = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.conv_layer = nn.ModuleList()\n",
    "        self.batch_norm = nn.ModuleList()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.to_feature_shape = nn.Linear(z_dim, np.prod(conv_options['feature_shape']))\n",
    "        self.unflatten = nn.Unflatten(1, conv_options['feature_shape'])\n",
    "\n",
    "        in_channels = conv_options['in_channels'][::-1]\n",
    "        kernel_sizes = conv_options['kernel_sizes'][::-1]\n",
    "        strides = conv_options['strides'][::-1]\n",
    "        padding = conv_options['padding'][::-1]\n",
    "\n",
    "        for i in range(conv_options['n_layers']):\n",
    "            layer = nn.LazyConvTranspose2d(\n",
    "                out_channels=in_channels[i],\n",
    "                kernel_size=kernel_sizes[i],\n",
    "                stride=strides[i],\n",
    "                padding=padding[i],\n",
    "                # fix ambiguity of conv2d output with stride > 1\n",
    "                output_padding=1 if strides[i] > 1 else 0\n",
    "            )\n",
    "            self.conv_layer.append(layer)\n",
    "            if batch_norm:\n",
    "                # Don't apply batch_norm on the last layer\n",
    "                if i + 1 < conv_options['n_layers']:\n",
    "                    C = in_channels[i]\n",
    "                    self.batch_norm.append(nn.BatchNorm2d(num_features=C))\n",
    "                    continue\n",
    "            self.batch_norm.append(nn.Identity())\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.to_feature_shape(z)\n",
    "        x = self.unflatten(x)\n",
    "\n",
    "        for i, convT2d in enumerate(self.conv_layer):\n",
    "            x = convT2d(x)\n",
    "            if i + 1 < len(self.conv_layer):\n",
    "                x = self.batch_norm[i](x)\n",
    "                x = self.relu(x)\n",
    "                x = self.drop_out(x)\n",
    "            else:\n",
    "                x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, out_channels, kernel_size=3, stride=1):\n",
    "        \"\"\"\n",
    "        Never resize the `out_channels` unless `stride` is greater than 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if stride > 1:\n",
    "            self.shortcut = nn.LazyConv2d(out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        self.conv_1 = nn.LazyConv2d(out_channels, kernel_size, stride=stride, padding=1)\n",
    "        self.conv_2 = nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.bn_1 = nn.LazyBatchNorm2d()\n",
    "        self.bn_2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x                 # residual\n",
    "        res = self.conv_1(res)\n",
    "        res = self.bn_1(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv_2(res)\n",
    "        res = self.bn_2(res)\n",
    "        x = self.shortcut(x)    # skip connection\n",
    "        y = res + x             # H(x) = F(x) + x\n",
    "        y = self.relu(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class InverseAutoregressiveFlow(Model):\n",
    "    def __init__(self, conv_options, z_dim, h_dim, r_loss_factor, batch_norm=True, dropout=0.2):\n",
    "        super().__init__(name=\"IAF\")\n",
    "        self.encoder = Encoder(conv_options, z_dim, h_dim, batch_norm, dropout)\n",
    "        self.sampler = GaussianSampler(z_dim)\n",
    "        self.decoder = Decoder(conv_options, z_dim, batch_norm, dropout)\n",
    "        self.r_loss_factor = r_loss_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sampler(mu, log_var)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = self.loss_fn(x, x_hat, mu, log_var)\n",
    "        return x_hat, loss\n",
    "\n",
    "    def loss_fn(self, x, x_hat, mu, log_var):\n",
    "        reconstruction_loss = torch.mean(torch.square(x_hat - x), dim=(1, 2, 3))\n",
    "        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=tuple(range(1, mu.ndim)))\n",
    "        return self.r_loss_factor * reconstruction_loss + kl_divergence\n",
    "\n",
    "\n",
    "model = InverseAutoregressiveFlow(\n",
    "    conv_options=CONV_OPTIONS,\n",
    "    z_dim=Z_DIM,\n",
    "    h_dim=H_DIM,\n",
    "    r_loss_factor=R_LOSS_FACTOR\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuvQTAXX5DmS",
    "outputId": "d45660e2-7f6d-4f25-b684-fd3ccbc178f4",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:14:50.089882Z",
     "start_time": "2025-10-26T01:14:50.071228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InverseAutoregressiveFlow(\n",
      "  (encoder): Encoder(\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "    (drop_out): Dropout2d(p=0.2, inplace=False)\n",
      "    (conv_layer): ModuleList(\n",
      "      (0): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1-2): 2 x LazyConv2d(0, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (3): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (batch_norm): ModuleList(\n",
      "      (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1-3): 3 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (mu): LazyLinear(in_features=0, out_features=32, bias=True)\n",
      "    (log_var): LazyLinear(in_features=0, out_features=32, bias=True)\n",
      "  )\n",
      "  (sampler): VariationalSampler()\n",
      "  (decoder): Decoder(\n",
      "    (activation): LeakyReLU(negative_slope=0.01)\n",
      "    (drop_out): Dropout2d(p=0.2, inplace=False)\n",
      "    (conv_layer): ModuleList(\n",
      "      (0): LazyConvTranspose2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LazyConvTranspose2d(0, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): LazyConvTranspose2d(0, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (3): LazyConvTranspose2d(0, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (batch_norm): ModuleList(\n",
      "      (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Identity()\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "    (to_feature_shape): Linear(in_features=32, out_features=3136, bias=True)\n",
      "    (unflatten): Unflatten(dim=1, unflattened_size=(64, 7, 7))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "tracker = Tracker(model=model)\n",
    "trainer = Trainer(model, tracker, train_data_loader, val_data_loader, test_data_loader, device=device)\n",
    "trainer.fit()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "RKRIYNG3iYh4",
    "outputId": "e46fc255-1d4b-4b56-811b-f219fedc81b9",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:22:51.041139Z",
     "start_time": "2025-10-26T01:14:51.424634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train loss: 39.52104]: : 1688it [00:12, 137.08it/s]                        \n",
      "[Test loss: 34.12180]: : 1688it [00:04, 342.95it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: inf --> 36.24759)\n",
      "Train loss: 47.27183\n",
      "Validate loss: 36.24759\n",
      "===============================\n",
      "\n",
      "Epoch 2/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 35.76083]: : 1688it [00:12, 136.19it/s]                        \n",
      "[Test loss: 32.68049]: : 1688it [00:06, 271.09it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 36.24759 --> 34.09967)\n",
      "Train loss: 37.22656\n",
      "Validate loss: 34.09967\n",
      "===============================\n",
      "\n",
      "Epoch 3/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train loss: 34.09849]: : 1688it [00:12, 139.23it/s]                        \n",
      "[Test loss: 31.89107]: : 1688it [00:06, 278.28it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 34.09967 --> 32.65034)\n",
      "Train loss: 35.53799\n",
      "Validate loss: 32.65034\n",
      "===============================\n",
      "\n",
      "Epoch 4/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 36.04593]: : 1688it [00:14, 119.20it/s]                        \n",
      "[Test loss: 31.45315]: : 1688it [00:06, 270.38it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 32.65034 --> 32.22507)\n",
      "Train loss: 34.60477\n",
      "Validate loss: 32.22507\n",
      "===============================\n",
      "\n",
      "Epoch 5/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 32.91176]: : 1688it [00:11, 153.41it/s]                        \n",
      "[Test loss: 33.68723]: : 1688it [00:04, 340.57it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 1 out of 3\n",
      "Train loss: 33.99758\n",
      "Validate loss: 32.22496\n",
      "===============================\n",
      "\n",
      "Epoch 6/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 31.11660]: : 1688it [00:11, 147.87it/s]                        \n",
      "[Test loss: 31.33584]: : 1688it [00:04, 337.61it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 32.22507 --> 31.09518)\n",
      "Train loss: 33.49625\n",
      "Validate loss: 31.09518\n",
      "===============================\n",
      "\n",
      "Epoch 7/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train loss: 34.93422]: : 1688it [00:11, 147.91it/s]                        \n",
      "[Test loss: 28.69375]: : 1688it [00:05, 307.97it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 31.09518 --> 30.96019)\n",
      "Train loss: 33.16609\n",
      "Validate loss: 30.96019\n",
      "===============================\n",
      "\n",
      "Epoch 8/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 33.24113]: : 1688it [00:11, 142.93it/s]                        \n",
      "[Test loss: 29.06083]: : 1688it [00:04, 341.89it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 1 out of 3\n",
      "Train loss: 32.89965\n",
      "Validate loss: 31.55124\n",
      "===============================\n",
      "\n",
      "Epoch 9/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 30.59657]: : 1688it [00:11, 148.26it/s]                        \n",
      "[Test loss: 31.77951]: : 1688it [00:05, 317.73it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 30.96019 --> 30.79113)\n",
      "Train loss: 32.61495\n",
      "Validate loss: 30.79113\n",
      "===============================\n",
      "\n",
      "Epoch 10/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 32.24335]: : 1688it [00:11, 144.11it/s]                        \n",
      "[Test loss: 27.38361]: : 1688it [00:04, 350.99it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 30.79113 --> 30.59121)\n",
      "Train loss: 32.45739\n",
      "Validate loss: 30.59121\n",
      "===============================\n",
      "\n",
      "Epoch 11/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 32.21578]: : 1688it [00:11, 149.22it/s]                        \n",
      "[Test loss: 30.98756]: : 1688it [00:05, 336.85it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 30.59121 --> 30.26175)\n",
      "Train loss: 32.27373\n",
      "Validate loss: 30.26175\n",
      "===============================\n",
      "\n",
      "Epoch 12/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 32.50203]: : 1688it [00:11, 147.23it/s]                        \n",
      "[Test loss: 25.89852]: : 1688it [00:04, 346.60it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 30.26175 --> 30.03311)\n",
      "Train loss: 32.10883\n",
      "Validate loss: 30.03311\n",
      "===============================\n",
      "\n",
      "Epoch 13/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 32.66205]: : 1688it [00:10, 155.21it/s]                        \n",
      "[Test loss: 30.74993]: : 1688it [00:04, 354.44it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 1 out of 3\n",
      "Train loss: 32.01613\n",
      "Validate loss: 30.25191\n",
      "===============================\n",
      "\n",
      "Epoch 14/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 37.60696]: : 1688it [00:11, 143.91it/s]                        \n",
      "[Test loss: 30.28551]: : 1688it [00:05, 335.80it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 30.03311 --> 29.98724)\n",
      "Train loss: 31.88461\n",
      "Validate loss: 29.98724\n",
      "===============================\n",
      "\n",
      "Epoch 15/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 30.82298]: : 1688it [00:11, 142.98it/s]                        \n",
      "[Test loss: 31.36378]: : 1688it [00:04, 360.22it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 29.98724 --> 29.76610)\n",
      "Train loss: 31.78623\n",
      "Validate loss: 29.76610\n",
      "===============================\n",
      "\n",
      "Epoch 16/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train loss: 31.52369]: : 1688it [00:11, 144.46it/s]                        \n",
      "[Test loss: 28.15342]: : 1688it [00:04, 350.06it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 29.76610 --> 29.51611)\n",
      "Train loss: 31.71532\n",
      "Validate loss: 29.51611\n",
      "===============================\n",
      "\n",
      "Epoch 17/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train loss: 32.79802]: : 1688it [00:11, 151.50it/s]                        \n",
      "[Test loss: 29.42417]: : 1688it [00:05, 319.97it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 1 out of 3\n",
      "Train loss: 31.61036\n",
      "Validate loss: 29.66283\n",
      "===============================\n",
      "\n",
      "Epoch 18/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 29.94351]: : 1688it [00:16, 102.73it/s]                        \n",
      "[Test loss: 28.19838]: : 1688it [00:09, 175.85it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 2 out of 3\n",
      "Train loss: 31.51767\n",
      "Validate loss: 29.85725\n",
      "===============================\n",
      "\n",
      "Epoch 19/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 31.46125]: : 1688it [00:18, 91.20it/s]                         \n",
      "[Test loss: 29.86788]: : 1688it [00:09, 186.51it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 29.51611 --> 29.46683)\n",
      "Train loss: 31.45978\n",
      "Validate loss: 29.46683\n",
      "===============================\n",
      "\n",
      "Epoch 20/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 29.23955]: : 1688it [00:18, 89.44it/s]                         \n",
      "[Test loss: 26.99115]: : 1688it [00:08, 208.26it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 1 out of 3\n",
      "Train loss: 31.37956\n",
      "Validate loss: 29.63823\n",
      "===============================\n",
      "\n",
      "Epoch 21/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 34.49886]: : 1688it [00:18, 89.74it/s]                         \n",
      "[Test loss: 29.63166]: : 1688it [00:09, 183.33it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to checkpoint... (Validate loss: 29.46683 --> 29.41381)\n",
      "Train loss: 31.31680\n",
      "Validate loss: 29.41381\n",
      "===============================\n",
      "\n",
      "Epoch 22/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 28.39809]: : 1688it [00:18, 90.51it/s]                         \n",
      "[Test loss: 33.11047]: : 1688it [00:08, 192.22it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 1 out of 3\n",
      "Train loss: 31.21849\n",
      "Validate loss: 29.54057\n",
      "===============================\n",
      "\n",
      "Epoch 23/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 32.41564]: : 1688it [00:18, 92.39it/s]                         \n",
      "[Test loss: 32.28059]: : 1688it [00:08, 202.71it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 2 out of 3\n",
      "Train loss: 31.19972\n",
      "Validate loss: 30.02030\n",
      "===============================\n",
      "\n",
      "Epoch 24/200\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train loss: 25.75775]: : 1688it [00:18, 90.18it/s]                         \n",
      "[Test loss: 29.23729]: : 1688it [00:08, 202.58it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early Stopping counter: 3 out of 3\n",
      "Train loss: 31.14254\n",
      "Validate loss: 29.49472\n",
      "===============================\n",
      "\n",
      "Early stopping now...\n",
      "Training complete! Elapsed time: 479.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T01:23:13.627833Z",
     "start_time": "2025-10-26T01:23:08.504257Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.test()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validate loss: 29.32512]: : 1688it [00:05, 331.55it/s]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 29.45660 \n",
      "\n",
      "Test complete! Elapsed time: 5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "id": "58ByLqmHwQPV"
   },
   "cell_type": "markdown",
   "source": "## Load model from disk"
  },
  {
   "metadata": {
    "id": "otOKo2nMxwYy",
    "ExecuteTime": {
     "end_time": "2025-10-26T01:23:16.149275Z",
     "start_time": "2025-10-26T01:23:16.104306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load from disk\n",
    "model = tracker.load_checkpoint()\n",
    "model.eval()\n",
    "\n",
    "# Relocate tensors to cpu\n",
    "device = 'cpu'\n",
    "trainer.device = device\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
